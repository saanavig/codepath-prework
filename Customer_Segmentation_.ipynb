{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgzxElPR5zGt"
      },
      "source": [
        "\n",
        "<b><h2><center>Unsupervised Learning Project - Bank Customer Segmentation</center></h2></b>\n",
        "<center><img src=\"https://copyassignment.com/wp-content/uploads/2021/07/1_iejTpHhx-u_R73XQD0GFkg.jpeg\" width=\"800\" height=\"400\"></center>\n",
        "\n",
        "### **Description**\n",
        "### **Context**\n",
        "\n",
        "Most banks have a large customer base - with different characteristics in terms of age, income, values, lifestyle, and more. Customer segmentation is the process of dividing a customer dataset into specific groups based on shared traits.\n",
        "\n",
        "According to a report from Ernst & Young, â€œA more granular understanding of consumers is no longer a nice-to-have item, but a strategic and competitive imperative for banking providers. Customer understanding should be a living, breathing part of everyday business, with insights underpinning the full range of banking operations.\n",
        "\n",
        "### **Objective**\n",
        "To identify different segments in the existing customer, based on their spending patterns as well as past interaction with the bank, using clustering algorithms, and provide recommendations to the bank on how to better market to and service these customers.\n",
        "\n",
        "### **Data Description**\n",
        "This dataset consists of 1 Million+ transaction by over 800K customers for a bank in India. The data contains information such as - customer age (DOB), location, gender, account balance at the time of the transaction, transaction details, transaction amount, etc.\n",
        "\n",
        "### **Data Dictionary:**\n",
        "     1. TransactionID: Transaction ID for every transaction\n",
        "     2. CustomerID: Unique ID for each customer\n",
        "     3. CustomerDOB: Customer Date of Birth\n",
        "     4. CustGender: Gender of the customer\n",
        "     5. CustLocation: Location of the customer\n",
        "     6. CustAccountBalance: Account Balance at the time of the transaction\n",
        "     7. TransactionDate: Date of the transaction\n",
        "     8. TransactionTime: Time of the transaction\n",
        "     9. TransactionAmount(INR): Transaction amount in Indian Rupees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnUc21J2Z15C"
      },
      "source": [
        "## **Fetching Data from kaggle website**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Wavfo4mQt3C8",
        "outputId": "d5578380-9c96-4922-dbc7-b797716c1af2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-307db0c7-0861-410d-91ac-3503638c44b8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-307db0c7-0861-410d-91ac-3503638c44b8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-5c2e8a8d365b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m     result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m             \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "BEhihwtsuCl8"
      },
      "outputs": [],
      "source": [
        "# install the kaggle API client\n",
        "!pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "vu6lnQzFuHOf",
        "outputId": "db6085e3-eb76-4d89-c35a-067db489a5b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "# The Kaggle API client expects this file to be in ~/.kaggle, so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNdd0RJiuhjn",
        "outputId": "71aede2d-49d2-4598-9914-2edfd6be69c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/bank_customer_segmentation/bank_customer_segmentation\n"
          ]
        }
      ],
      "source": [
        "! mkdir bank_customer_segmentation\n",
        "%cd bank_customer_segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mab0qTBes1lo",
        "outputId": "0c092429-3d25-49e3-a8bf-c9e2b075d3fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 403, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d shivamb/bank-customer-segmentation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p-4PJDicvYaR",
        "outputId": "42576468-faed-4474-dcc4-d6f00929282f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open bank-customer-segmentation.zip, bank-customer-segmentation.zip.zip or bank-customer-segmentation.zip.ZIP.\n"
          ]
        }
      ],
      "source": [
        "# # Unzipping downloaded file and removing unusable file\n",
        "!unzip bank-customer-segmentation.zip -d bank_customer_segmentation\n",
        "# !rm content/bank_customer_segmentation/bank-customer-segmentation.zip\n",
        "\n",
        "# !unzip bank_customer_segmentation.zip\n",
        "# !unzip /content/bank_customer_segmentation/bank-customer-segmentation.zip -d bank_customer_segmentation\n",
        "!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IJCTt3BxHbF"
      },
      "source": [
        "## **Exploratory Data Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK53NJXEa4uE"
      },
      "source": [
        "**Importing necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q-VxkZktslvB"
      },
      "outputs": [],
      "source": [
        "import datetime as dt\n",
        "# Libraries to help with reading and manipulating data\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Libraries to help with data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# Removes the limit for the number of displayed columns\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "# Sets the limit for the number of displayed rows\n",
        "pd.set_option(\"display.max_rows\", 200)\n",
        "\n",
        "# to scale the data using z-score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# to compute distances\n",
        "from scipy.spatial.distance import pdist\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "# to perform hierarchical clustering, compute cophenetic correlation, and create dendrograms\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage, cophenet\n",
        "\n",
        "# to perform k-means clustering and compute silhouette scores\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# to visualize the elbow curve and silhouette scores\n",
        "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
        "\n",
        "# to suppress warnings\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v4hlzxxCyFzn",
        "outputId": "6153b33d-61fa-4e2e-bfb6-efe56e02dc70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/bank_customer_segmentation/bank-customer-segmentation.zip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-385bf5cf6e9e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# loading the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/bank_customer_segmentation/bank-customer-segmentation.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# \"Union[str, BaseBuffer]\"; expected \"Union[Union[str, PathLike[str]],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# ReadBuffer[bytes], WriteBuffer[bytes]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             handle = _BytesZipFile(\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcompression_args\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, archive_name, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;31m# Union[str, PathLike[str]], ReadBuffer[bytes], WriteBuffer[bytes]]\";\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;31m# expected \"Union[Union[str, PathLike[str]], IO[bytes]]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minfer_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1249\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilemode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mfilemode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodeDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/bank_customer_segmentation/bank-customer-segmentation.zip'"
          ]
        }
      ],
      "source": [
        "# loading the dataset\n",
        "data = pd.read_csv('/content/bank_customer_segmentation/bank-customer-segmentation.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODyc3YRpbdqz"
      },
      "outputs": [],
      "source": [
        "# Checking the shape of the dataset\n",
        "print('The dataset has',data.shape[0], 'rows and', data.shape[1],'columns')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yr7Pn0AfyON6"
      },
      "outputs": [],
      "source": [
        "# check for first 5 rows of the data\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zq45swpBdCuc"
      },
      "outputs": [],
      "source": [
        "# copying the data to another variable to avoid any changes to original data\n",
        "df = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kc0NHydqyU3O"
      },
      "outputs": [],
      "source": [
        "# let's look at the structure of the data\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yEc2Yfce7Bx"
      },
      "outputs": [],
      "source": [
        "## Show details data set\n",
        "def check(df):\n",
        "    l=[]\n",
        "    columns=df.columns\n",
        "    for col in columns:\n",
        "        dtypes=df[col].dtypes\n",
        "        nunique=df[col].nunique()\n",
        "        sum_null=df[col].isnull().sum()\n",
        "        l.append([col,dtypes,nunique,sum_null, num_duplicates])\n",
        "    df_check=pd.DataFrame(l)\n",
        "    df_check.columns=['column','dtypes','nunique','sum_null']\n",
        "    return df_check\n",
        "check(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GqxwNq03ARh"
      },
      "source": [
        "**Observations:**\n",
        "- This dataset has 3 numerical columns and 6 categorical columns.\n",
        "- CustomerDOB and transaction date columns should be converted into date type.\n",
        "- We can delete the missing values as we have very less missing data.\n",
        "- We can calculate the age of the customer by subtraction customer DOB from Transaction date.\n",
        "- We can delele the Transaction Time column as we don't need time of transaction.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCDyFnka2aBJ"
      },
      "outputs": [],
      "source": [
        "# delete missing data\n",
        "df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn94QipJglWX"
      },
      "outputs": [],
      "source": [
        "#check for duplication\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA4nxwBigoTH"
      },
      "outputs": [],
      "source": [
        "# convert type of columns TransactionDate,CustomerDOB from string to datetime\n",
        "df['TransactionDate'] = pd.to_datetime(df['TransactionDate'])\n",
        "df['CustomerDOB'] = pd.to_datetime(df['CustomerDOB'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob1QhNz2jzpm"
      },
      "outputs": [],
      "source": [
        "df['TransactionMonth'] = df.TransactionDate.dt.month\n",
        "df['TransactionMonthName'] = df.TransactionDate.dt.month_name()\n",
        "df['TransactionDay'] = df.TransactionDate.dt.day\n",
        "df['TransactionDayName'] = df.TransactionDate.dt.day_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MaCFAI2grcJ"
      },
      "outputs": [],
      "source": [
        "# let's check the data types of TransactionDate and CustomerDOB\n",
        "df.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLOHm5ohbUha"
      },
      "source": [
        "#### Performing Recency, Frequency and Monetary analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCkCCtsObkdz"
      },
      "outputs": [],
      "source": [
        "# Calculate the recency, frequency and monetary value for each customer\n",
        "snapshot_date = df['TransactionDate'].max() + dt.timedelta(days=1)\n",
        "df_rfm = df.groupby('CustomerID').agg({\n",
        "    'TransactionDate': lambda x: (snapshot_date - x.max()).days,\n",
        "    'TransactionID': 'count',\n",
        "    \"TransactionAmount (INR)\": 'sum'\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMJz0pkmckCz"
      },
      "outputs": [],
      "source": [
        "df_rfm.rename(columns={\n",
        "    'TransactionDate': 'recency',\n",
        "    'TransactionID': 'frequency',\n",
        "    \"TransactionAmount (INR)\": 'monetary_value',\n",
        "}, inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7CGgPIwpdiEH"
      },
      "outputs": [],
      "source": [
        "# Create bins and labels for each RFM score\n",
        "r_bins = [0, 30, 60, 90, df_rfm['recency'].max()]\n",
        "f_bins = [0, 1, 2, 3, 4, 5, df_rfm['frequency'].max()]\n",
        "m_bins = [0, 100, 500, 1000, df_rfm['monetary_value'].max()]\n",
        "r_labels = [4, 3, 2, 1]\n",
        "f_labels = [1, 2, 3, 4, 5, 6]\n",
        "m_labels = [1, 2, 3, 4]\n",
        "df_rfm['r_score'] = pd.cut(df_rfm['recency'], bins=r_bins, labels=r_labels, include_lowest=True)\n",
        "df_rfm['f_score'] = pd.cut(df_rfm['frequency'], bins=f_bins, labels=f_labels, include_lowest=True)\n",
        "df_rfm['m_score'] = pd.cut(df_rfm['monetary_value'], bins=m_bins, labels=m_labels, include_lowest=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_14Vd7RlexeX"
      },
      "outputs": [],
      "source": [
        "# Combine the RFM scores to create a single RFM score\n",
        "df_rfm['RFM'] = df_rfm['r_score'].astype(str) + df_rfm['f_score'].astype(str) + df_rfm['m_score'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4rnqC5hd9MC"
      },
      "outputs": [],
      "source": [
        "# Assign customer segments based on the RFM score\n",
        "def get_segment(x):\n",
        "    if x in ['111', '112', '113', '114', '115', '116', '117']:\n",
        "        return 'High-value'\n",
        "    elif x in ['121', '122', '123', '124', '125', '126', '127']:\n",
        "        return 'Mid-value'\n",
        "    else:\n",
        "        return 'Low-value'\n",
        "df_rfm['segment'] = df_rfm['RFM'].apply(get_segment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9q7_wiOmfcZz"
      },
      "outputs": [],
      "source": [
        "#Combine the segment column with the main dataframe\n",
        "segment = df_rfm['segment']\n",
        "\n",
        "# Merge the dataframes based on CustomerID\n",
        "df = pd.merge(df, segment, on='CustomerID')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5u3KuCLiFYb"
      },
      "source": [
        "**Calculating the age of the customer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5OeuMuRg-LH"
      },
      "outputs": [],
      "source": [
        "# let's calculate the customer age using Transaction date and Customer Date of Birth\n",
        "df['CustomerAge'] =df['TransactionDate'].dt.year - df['CustomerDOB'].dt.year"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmChaHUwidg3"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDgim9rz-L3E"
      },
      "source": [
        "- There are negative age seen in the customer age column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sehYRGGp-asm"
      },
      "source": [
        "**Analysing the CustomerAge column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QXevoaxU2mSJ"
      },
      "outputs": [],
      "source": [
        "df[df['CustomerAge'] < 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-KQP7H1-V_S"
      },
      "source": [
        "- We can drop the negative age data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx6Gszxa3wy6"
      },
      "outputs": [],
      "source": [
        "df['TransactionDate'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EY63bcW_4VjP"
      },
      "outputs": [],
      "source": [
        "df['TransactionDate'].min()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bep5zC8t_MnF"
      },
      "source": [
        "- We have transactions of one year in our dataset which starts from January 2016 and ends in December 2016."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7L8YS5w4dWu"
      },
      "outputs": [],
      "source": [
        "df['CustomerAge'].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLIFDlr54l0f"
      },
      "outputs": [],
      "source": [
        "df['CustomerAge'].max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VCWxI1wAgY1"
      },
      "source": [
        "- We can drop the data for which the customer has age greater than 100 and age less than 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FPQRbsa_mRM"
      },
      "outputs": [],
      "source": [
        "# Filter rows where CustomerAge is greater than 100 and CustomerAge less than 0\n",
        "filtered_df_1 = df[df['CustomerAge'] > 100]\n",
        "filtered_df_2 = df[df['CustomerAge'] < 0]\n",
        "\n",
        "# Combine the indices of filtered DataFrames\n",
        "indices_to_drop = filtered_df_1.index.union(filtered_df_2.index)\n",
        "\n",
        "# Drop the resulting rows from the original DataFrame\n",
        "df.drop(indices_to_drop, inplace=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0eOfSgyBb1E"
      },
      "source": [
        "**Droping the data for age less than zero and age greater than 100 in CustomerAge column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0I_8feFRjPn1"
      },
      "outputs": [],
      "source": [
        "df['CustomerAge'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64V15-APCqh5"
      },
      "source": [
        "**Droping the TransactionTime column**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjUgCSoQk3uR"
      },
      "outputs": [],
      "source": [
        "# deleting the TransactionTime column\n",
        "# df.drop(columns=['TransactionTime'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFV7Cq_endQH"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAhniH5VngTX"
      },
      "outputs": [],
      "source": [
        "# let's check the count of gender\n",
        "df.CustGender.value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlxtcLtP1q1n"
      },
      "outputs": [],
      "source": [
        "df.CustGender.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfMJkxhGn2UA"
      },
      "source": [
        "- Majority of Customer are Male customer in this data set.\n",
        "- The data set has 72% of Male customer and 27% of Female customer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WzZVdPvV2Gp8"
      },
      "outputs": [],
      "source": [
        "df['CustAccountBalance'].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9MNO3dJ7nbZ"
      },
      "outputs": [],
      "source": [
        "df.groupby(by='CustLocation')['CustAccountBalance'].sum().sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fCCsb0k9BGS"
      },
      "outputs": [],
      "source": [
        "df[df['CustAccountBalance'] == 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjy-N6Hj9UzJ"
      },
      "source": [
        "- There are 1614 customer with zero balance in their account"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-AKh0YM9f3b"
      },
      "outputs": [],
      "source": [
        "df[df['TransactionAmount (INR)'] > 1000000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt6vnVyx9xIe"
      },
      "source": [
        "- There are only 2 transaction above 1 million."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mn3S_uac-X9T"
      },
      "outputs": [],
      "source": [
        "df['CustLocation'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "InHMQpM0nR_2"
      },
      "outputs": [],
      "source": [
        "cities = df['CustLocation'].unique().tolist()\n",
        "print(cities)\n",
        "print(len(cities))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80uRwMA6-tgz"
      },
      "source": [
        "- Metro cities like Mumbai, Bangalore, New Delhi has more transactions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5prF3VTc8WfY"
      },
      "outputs": [],
      "source": [
        "df.groupby(by='CustLocation')['CustAccountBalance'].max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kywk3Gibw6iU"
      },
      "source": [
        "### Location Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLCmOH4xyuTd"
      },
      "outputs": [],
      "source": [
        "indian_cities = pd.read_csv('/content/indian_cities.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5o5uaRIy0XT"
      },
      "outputs": [],
      "source": [
        "indian_cities.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0_AaDXG2AQ3"
      },
      "outputs": [],
      "source": [
        "# def clean_text(text):\n",
        "#     stripped_text = ''.join(c for c in text if c.isalpha())\n",
        "#     lowercase_text = stripped_text.lower()\n",
        "#     return lowercase_text\n",
        "import unicodedata\n",
        "\n",
        "def clean_text(text):\n",
        "    # normalize the text using NFKD method\n",
        "    normalized_text = unicodedata.normalize('NFKD', text)\n",
        "\n",
        "    # remove any non-alphabetic characters\n",
        "    stripped_text = ''.join(c for c in normalized_text if c.isalpha())\n",
        "\n",
        "    # convert the text to lowercase\n",
        "    lowercase_text = stripped_text.lower()\n",
        "\n",
        "    return lowercase_text\n",
        "indian_cities['city'] = indian_cities['city'].apply(clean_text)\n",
        "indian_cities['admin_name'] =  indian_cities['admin_name'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2kL2NKly3Oj"
      },
      "outputs": [],
      "source": [
        "states_map = dict(zip(indian_cities['city'], indian_cities['admin_name']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7aIG7niTb2o"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9RQtuR1TQHo"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "import re\n",
        "# Instantiate a BertTokenizer object\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define a function to remove numeric tokens from a string\n",
        "def remove_numeric_tokens(text):\n",
        "    # Tokenize the text using the BertTokenizer\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    # Remove numeric tokens\n",
        "    tokens = [token for token in tokens if not token.isnumeric()]\n",
        "    # Convert the list of tokens back to a string\n",
        "    text = tokenizer.convert_tokens_to_string(tokens)\n",
        "    return text\n",
        "\n",
        "# Apply the remove_numeric_tokens function to the CustLocation column\n",
        "df['CustLocation'] = df['CustLocation'].apply(remove_numeric_tokens)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3J8b_aFz9Rn"
      },
      "outputs": [],
      "source": [
        "# define a function to classify addresses based on city name\n",
        "# def classify_address(address):\n",
        "#     for city in states_map.keys():\n",
        "#         if city in address:\n",
        "#             return states_map[city]\n",
        "#     return 'Unknown'\n",
        "\n",
        "def classify_address(address):\n",
        "    for city, state in states_map.items():\n",
        "        if city in address:\n",
        "            return state\n",
        "        # check if any words in address match a city with a slightly different name\n",
        "        for word in address.split():\n",
        "            if word in city.split():\n",
        "                return state\n",
        "    return 'Unknown'\n",
        "\n",
        "# classify the given addresses\n",
        "df['State/Union'] = df['CustLocation'].apply(classify_address)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YybTLZau4fFz"
      },
      "outputs": [],
      "source": [
        "df['State/Union'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0_ji2F45hi-"
      },
      "outputs": [],
      "source": [
        "unknown_cities_mask = df['State/Union'] == 'Unknown'\n",
        "unknown_cities_df = df.loc[unknown_cities_mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9jiio-06lwt"
      },
      "outputs": [],
      "source": [
        "unknown_cities_index = df.loc[unknown_cities_mask].index\n",
        "df.drop(index=unknown_cities_index, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bySkmXdZ6EUp"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcyEeEJ_p7YK"
      },
      "source": [
        "## **Univariate Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDR-FrxDsfWw"
      },
      "outputs": [],
      "source": [
        "# assigning the numerical columns and categorical columns\n",
        "num_col = df.select_dtypes(include=np.number)\n",
        "cat_col = df.select_dtypes(exclude=np.number)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QLJvkq0tTfS"
      },
      "outputs": [],
      "source": [
        "# plotting boxplot for numerical columns\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "plt.figure(figsize=(30,30))\n",
        "for index,column in enumerate(num_col):\n",
        "    plt.subplot(7,4,index+1)\n",
        "    sns.boxplot(data=num_col,x=column)\n",
        "\n",
        "plt.tight_layout(pad = 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49juKOmSu5fx"
      },
      "source": [
        "**Observations**:\n",
        "- CusAccountBalance and TransactionAmount(INR) has outliers, but they are not ouliers in our case as there is no limit in having balance and transaction amount.\n",
        "- CutomerAge has outliers in left side indicates that they are younger children age."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQyfj8oww9f5"
      },
      "outputs": [],
      "source": [
        "# ploting countplot for custGender\n",
        "sns.countplot(data = df, x ='CustGender')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vj94mw_GUoi"
      },
      "source": [
        "- We already know that we have majority of male customers when compared to female customers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esKnj0JpzFl9"
      },
      "source": [
        "##**Multivariate Analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkbQ-TffGfkq"
      },
      "outputs": [],
      "source": [
        "# ploting boxplot for CustGender and CustomerAge\n",
        "sns.boxplot(data=df, x='CustGender', y='CustomerAge') ;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VlS2ZyRG-Lq"
      },
      "source": [
        "- Most of the customers in male and female are in age range of 22 years to 32 years.\n",
        "- Both female and male customer age has children accounts which are outlier at the bottom."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFrHXQ7jKN6j"
      },
      "outputs": [],
      "source": [
        "# ploting boxplot for CustGender and CustAccountBalance\n",
        "sns.boxplot(data=df, x='CustGender', y='CustAccountBalance') ;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mpP2YiegN4b"
      },
      "source": [
        "- We have outliers in both female and male customer account balance which denotes that lot of customers has high account balance in their account."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b4itryLG9tJ"
      },
      "outputs": [],
      "source": [
        "df['TransactionDate'].dt.month.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLWQRulpIeVF"
      },
      "source": [
        "**Creating new column as TransactionMonth as we have data of one year transaction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UybeEVrgIUo3"
      },
      "outputs": [],
      "source": [
        "# extracting the month from Transaction date column\n",
        "df['TransactionMonth'] = df['TransactionDate'].dt.month"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPEhQommIr6U"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0m8cli-Gffn"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Grkek2r1GfcX"
      },
      "outputs": [],
      "source": [
        "# ploting line plot for TransactionMonth and TransactionAmount\n",
        "sns.lineplot(data = df , x = 'TransactionMonth' , y = 'TransactionAmount (INR)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awOptlHMhPUX"
      },
      "source": [
        "- This lineplot shows that customers had higher transaction amount in the month of April followed by June."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hm8OvffBGfGI"
      },
      "outputs": [],
      "source": [
        "#Ploting lineplot for TransactionMonth and TransactionAmount\n",
        "sns.lineplot(data = df, x='TransactionMonth', y='TransactionAmount (INR)', ci=False, hue='CustGender');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STZ3NYGehn-n"
      },
      "source": [
        "- The transaction amount of female customers is higher than male customers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rNrgTKfLPtr"
      },
      "outputs": [],
      "source": [
        "# ploting lineplot for TransactionMonth and CustAccountBalance\n",
        "sns.lineplot(data = df , x = 'TransactionMonth' , y = 'CustAccountBalance');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkF3BV62h3fZ"
      },
      "source": [
        "- Customers account balance is higher in August month followed my November."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfhf0_S3JpPS"
      },
      "outputs": [],
      "source": [
        "#ploting lineplot for TransactionMonth and CustAcoountBalance\n",
        "sns.lineplot(data = df, x='TransactionMonth', y='CustAccountBalance', ci=False, hue='CustGender');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enE1Rlj9iE9F"
      },
      "source": [
        "- Most of the female customers has high balance in their account when compared with male customers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv1xJvnVLvy4"
      },
      "outputs": [],
      "source": [
        "# Ploting scatterplot for CustomerAge and TransactionAmount\n",
        "sns.scatterplot(data=df, x='CustomerAge', y='TransactionAmount (INR)');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYYgunp4iYZK"
      },
      "source": [
        "- Positive correlation is seen as the age increases transaction amount increases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvzEmvPMMHyM"
      },
      "outputs": [],
      "source": [
        "# ploting scatter between CustomerAge and CustAccountBalance\n",
        "sns.scatterplot(data=df, x='CustomerAge', y='CustAccountBalance');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfXofWe5j0B_"
      },
      "source": [
        "- same as previous plot, we can see positive correlation as the age of the customer increases account balance of the customer also increases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSa6ECk9xRa4"
      },
      "outputs": [],
      "source": [
        "# plt.figure(figsize=(40,7))\n",
        "# sns.pairplot(data = df ,hue='CustGender');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ex_R-22al44l"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSAXzQo5kj5L"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(data=df[['CustomerAge','TransactionMonth','TransactionAmount (INR)','CustAccountBalance']].corr(), annot=True);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmUs-J90moKS"
      },
      "source": [
        "- There are no strong correlation seen between the variables\n",
        "- Customer age has some correlation with  customer account balance and transaction amount."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJNO2yQlZVrs"
      },
      "source": [
        "## **Inights form EDA**\n",
        "- There are 1048567 observations and 9 columns\n",
        "- We have missing values in 4 columns which are CustomerDOB, CustGender, CustLocation, CustAccountBalance.\n",
        "- Missing values are dropped as the missing values are less compared to our dataset.\n",
        "- Customer Age is calculated using Transaction Date and Date of the birth of the customer\n",
        "- Majority of the customers are Male. The data set has 72% of Male customer and 27% of Female customer.\n",
        "- Most of the customers in male and female are in age range of 22 years to 32 years.\n",
        "- customers had higher transaction amount in the month of April followed by June.\n",
        "- The transaction amount of female customers is higher than male customers.\n",
        "- Customers account balance is higher in August month followed my November.\n",
        "- Most of the female customers has high balance in their account when compared with male customers.\n",
        "- There are no strong correlation seen between the variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDZcFqsy005c"
      },
      "source": [
        "## **Data Preparation**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXRbW7w41XWP"
      },
      "source": [
        "- We can delete unnecessary columns:\n",
        "  - Transaction ID is unique for every transaction, so we can delete the column\n",
        "  - We have transaction data for the year 2016, already we extracted the month of the transaction. So we can delete the transaction date column in our data.\n",
        "  - We calculated age of our customer using customer date of birth and transaction date. So we can delete the customer date of birth column.\n",
        "  - We already deleted transaction time column.\n",
        "\n",
        "- Missing values are less as we have 1 Million observations, so we dropped our missing values.\n",
        "- Outliers found in age, account balance and transaction account. We are not going to treat the outliers for this data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkB6gleqD8gt"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C1nXK0V_1Lye"
      },
      "outputs": [],
      "source": [
        "del_col = ['CustomerID', 'TransactionID', 'CustomerDOB', 'TransactionDate', 'TransactionTime', 'CustLocation']\n",
        "df.drop(columns=del_col, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-3qLV7pERTq"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXZFlEtNEShS"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UI4tZVgEUAy"
      },
      "outputs": [],
      "source": [
        "df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fm8nQet1FYsn"
      },
      "outputs": [],
      "source": [
        "# df.drop(columns='CustomerID', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBiKNvi_JZO0"
      },
      "outputs": [],
      "source": [
        "df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnytXbgdJaq7"
      },
      "outputs": [],
      "source": [
        "df.to_excel('Clean_bank_transaction.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIyM0z6OAOHQ"
      },
      "source": [
        "# Data Modelling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FxgjI3QFki0R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NK4jp4SpAUeH"
      },
      "outputs": [],
      "source": [
        "df = pd.read_excel('/content/bank_customer_segmentation/Clean_bank_transaction.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNK4JlZyBzQY"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEudF9SSB2oj"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-vBqEK8gwBH"
      },
      "outputs": [],
      "source": [
        "# df.drop('CustLocation', inplace=True, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZL1wFCLBqfk"
      },
      "outputs": [],
      "source": [
        "check(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SstUtweJtk-7"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create an instance of the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Fit the encoder to the CustGender column in the DataFrame\n",
        "label_encoder.fit(df['CustGender'])\n",
        "\n",
        "# Transform the CustGender column using the fitted encoder\n",
        "df['CustGender'] = label_encoder.transform(df['CustGender'])\n",
        "\n",
        "# Fit the encoder to the segment column in the DataFrame\n",
        "label_encoder.fit(df['segment'])\n",
        "\n",
        "# Transform the segment column using the fitted encoder\n",
        "df['segment'] = label_encoder.transform(df['segment'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWd2vhPlvasa"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sOIGLfI8Cty"
      },
      "outputs": [],
      "source": [
        "# Perform one-hot encoding for the 'State' column\n",
        "ohe_df = pd.get_dummies(df['State/Union'], prefix='State')\n",
        "\n",
        "# Add the one-hot encoded columns to the original DataFrame\n",
        "df = pd.concat([df, ohe_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3OEN4IMQ8ITO"
      },
      "outputs": [],
      "source": [
        "df.drop('State/Union', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cKdAL_DF8YaB"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDksGz7ywI8z"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "scaler = StandardScaler()\n",
        "df_std = scaler.fit_transform(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1idHbd3mlH9H"
      },
      "outputs": [],
      "source": [
        "type(df_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1iOaIITTqjE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "# Setting up TPU strategy\n",
        "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(tpu)\n",
        "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "strategy = tf.distribute.TPUStrategy(tpu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzFQV5TWVVxW"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "\n",
        "# Define the range of cluster numbers to test\n",
        "n_clusters = range(2, 8)\n",
        "\n",
        "# Initialize a list to store the silhouette scores for each cluster number\n",
        "scores = []\n",
        "\n",
        "# Loop through each cluster number and compute the corresponding silhouette score\n",
        "for n in n_clusters:\n",
        "    with strategy.scope():\n",
        "        print(f'Running k-means with {n} clusters...')\n",
        "        start_time = time.time()\n",
        "        kmeans = KMeans(n_clusters=n, random_state=42)\n",
        "        kmeans.fit(df)\n",
        "        score = calinski_harabasz_score(df, kmeans.labels_)\n",
        "        scores.append(score)\n",
        "        end_time = time.time()\n",
        "        print(f'Time spent: {end_time - start_time} seconds')\n",
        "        print()\n",
        "\n",
        "# Choose the optimal number of clusters based on the Calinski-Harabasz score\n",
        "optimal_n_clusters = n_clusters[np.argmax(scores)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2l4KLSYdr_G"
      },
      "outputs": [],
      "source": [
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0I6q5H-Vokw"
      },
      "outputs": [],
      "source": [
        "# Plot the Calinski-Harabasz score as a function of the number of clusters\n",
        "plt.plot(n_clusters, scores, 'bx-')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Calinski-Harabasz Score')\n",
        "plt.title('Elbow Plot')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yt5iSO6CWveM"
      },
      "outputs": [],
      "source": [
        "print(f'Optimal number of clusters: {optimal_n_clusters}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQY1kJNZXLWc"
      },
      "outputs": [],
      "source": [
        "# Fit the data to the optimal number of clusters\n",
        "with strategy.scope():\n",
        "    kmeans = KMeans(n_clusters=optimal_n_clusters, random_state=42)\n",
        "    kmeans.fit(df)\n",
        "\n",
        "# Assign each data point to a cluster\n",
        "labels = kmeans.predict(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2zYadE_e7Ok"
      },
      "outputs": [],
      "source": [
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4M-qnVpYBLI"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "scatter = ax.scatter(df.values[:, 0], df.values[:, 1], c=labels, cmap='viridis')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title(f'Clusters with k={optimal_n_clusters}')\n",
        "legend1 = ax.legend(*scatter.legend_elements(),\n",
        "                    loc=\"upper right\", title=\"Clusters\")\n",
        "ax.add_artist(legend1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDtVY6eDY3AL"
      },
      "outputs": [],
      "source": [
        "# Show the data in the different clusters\n",
        "# for i in range(optimal_n_clusters):\n",
        "#     print(f'Cluster {i+1} has {len(df[labels == i])} data points:')\n",
        "#     print(df[labels == i])\n",
        "#     print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGw1RHSngDz_"
      },
      "outputs": [],
      "source": [
        "def plot_col_cluster(column, df=df):\n",
        "  for col in column:\n",
        "    plt.figure(figsize=(20,10))\n",
        "    plt.plot(df[col][labels==0], label='Cluster 1')\n",
        "    plt.plot(df[col][labels==1], label='Cluster 2')\n",
        "    plt.plot(df[col][labels==2], label='Cluster 3')\n",
        "    plt.plot(df[col][labels==3], label='Cluster 4')\n",
        "    plt.plot(df[col][labels==4], label='Cluster 5')\n",
        "    plt.plot(df[col][labels==5], label='Cluster 6')\n",
        "    plt.plot(df[col][labels==6], label='Cluster 7')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5OvTmkGViEo3"
      },
      "outputs": [],
      "source": [
        "columns = df.columns.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlC2xNLQiay1"
      },
      "outputs": [],
      "source": [
        "# plot_col_cluster(columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEi6S_5fk7gz"
      },
      "source": [
        "Hierarchical Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25O_gQT9yvbz"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# # Create a DataFrame with some data\n",
        "# # df = pd.DataFrame({\n",
        "# #     'A': [1, 2, 3, 5, 7, 8, 9, 10],\n",
        "# #     'B': [10, 20, 30, 40, 50, 60, 70, 80]\n",
        "# # })\n",
        "\n",
        "# # Compute the linkage matrix\n",
        "# Z = linkage(df, 'ward')\n",
        "\n",
        "# # Plot the dendrogram\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# dendrogram(Z, leaf_rotation=90., leaf_font_size=8.)\n",
        "# plt.xlabel('Samples')\n",
        "# plt.ylabel('Distance')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wFK4E6e7qbD"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  # Normalize the data\n",
        "  df_norm = (df - df.mean()) / df.std()\n",
        "\n",
        "  # Compute the linkage matrix\n",
        "  Z = linkage(df_norm[:5000], 'ward')\n",
        "\n",
        "  # Plot the dendrogram\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  dendrogram(Z, leaf_rotation=90., leaf_font_size=8.)\n",
        "  plt.xlabel('Samples')\n",
        "  plt.ylabel('Distance')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kT6mzZ233Wn4"
      },
      "outputs": [],
      "source": [
        "# Extract cluster labels\n",
        "from scipy.cluster.hierarchy import fcluster\n",
        "max_dist = 10\n",
        "cluster_labels = fcluster(Z, max_dist, criterion='distance')\n",
        "\n",
        "# Convert cluster labels to DataFrame\n",
        "cluster_labels = pd.DataFrame(cluster_labels, columns=['ClusterLabel'])\n",
        "\n",
        "# Concatenate cluster labels with original data\n",
        "df_clustered = pd.concat([df_norm, cluster_labels], axis=1)\n",
        "with strategy.scope():\n",
        "  # Visualize the distribution of data points within each cluster\n",
        "  sns.set_style('whitegrid')\n",
        "  g = sns.PairGrid(df_clustered[:5000], hue='ClusterLabel', palette='Set2')\n",
        "  g.map_diag(plt.hist)\n",
        "  g.map_offdiag(plt.scatter, alpha=0.7)\n",
        "  g.add_legend()\n",
        "  plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}